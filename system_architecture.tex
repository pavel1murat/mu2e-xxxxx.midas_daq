\section{System architecture}

The described system consists of two major software components :
\begin{itemize}
\item
  ARTDAQ, which handles the trigger farm, the data processing,
  and all high-bandwidth data traffic
\item
  MIDAS, which handles the run configuration, run control, messaging,
  standardizes interfaces between various parts of the online system,
  and provides web-based interfaces for shifters and experts,
\end{itemize}

The architecture of the proposed DAQ system is schematically presented
in Figure ~\ref{figure:system_architecture} and explained in the remaining
part of this section.

\begin{figure}[H]
  \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] at (0,0.) {
      % \node[shift={(0 cm,0.cm)},inner sep=0,rotate={90}] at (0,0) {}
      \makebox[\textwidth][c] {
        \includegraphics[width=1.1\textwidth]{pdf/system_architecture}
      }
    };
    % \node [text width=8cm, scale=1.0] at (14.5,0.5) {$\mu_B$, expected background mean};
    % \node [text width=8cm, scale=1.0, rotate={90}] at (1.5,7.5) { $S_{D}$, ``discovery'' signal strength  };
  \end{tikzpicture}
  \caption{
    \label{figure:system_architecture}
    Architecture of the MIDAS-based DAQ system. Communication protocols:
    red solid lines - JSON-RPC, blue solid lines - XML-RPC,
    red dashed lines - http(s).
  }
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MIDAS}

The core parts of MIDAS are :

\begin{itemize}
\item
  {\bf Online DataBase (ODB)} : a shared memory segment, which stores configuration and slow
  control data of the system.
  Despite its name, ODB is not a database. Instead, it stores only current configuration
  of the system, As the new run starts, a run number-stamped configuration is exported and stored.
  Logical organization of ODB maps onto a json structure and is well suited for describing
  complex run and detector configurations with multiple levels of hierarchy.
  ODB also stores the slow control data and provides efficient access to them to multiple
  DAQ clients.
\item 
  {\bf mhttpd}: a web server with restricted functionality.
  {\bf mhttpd} serves as an interface to ODB for all clients and provides
  a web-based interface to ODB as well as a unified web-based run control interface.  
  {\bf mhttpd} doesn't execute any applications. Instead, it only updates the ODB.
\item
  {\bf mlogger}: an executable responsible for logging the data. In case of Mu2e, that
  is the slow control data.
  {\bf mlogger} supports multiple backends, including local files, MySQL and PGSDL databases
\item
  {\bf mserver}: an executable handling communication with remote clients
\item
  {\bf sequencer}: an interactive tool handling the detector hardware configuration.
  The Sequencer has interfaces to ODB, operating system, and a simple scripting language
  which provides the level of flexibility needed for configuring a detector of any complexity
  at begin run.
\item
  {\bf clients} : frontends and other user analysis executables, communicating with the
  hardware and other parts of the system. MIDAS supports multiple types of frontends,
  Mu2e only needs the slow control functioinality.
\item
  MIDAS state machine has four transitions: {\bf start}, {\bf stop}, {\bf pause},
  and {\bf resume}.
  Combined with the Sequencer-based hardware configuration tools,
  that provides all functionality a DAQ state machine might need.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interfaces to ODB}

MIDAS ODB has several interfaces:
\begin{itemize}
\item 
  web-based interface - via the mhttpd "ODB" page
\item
  odbedit: a command line interface
\item
  C/C++ API
\item
  python API
\end{itemize}

In all cases, clients connect to MHTTPD, and MHTTPD acts as the only agent
interacting with the ODB directly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Frontends}

\begin{itemize}
\item 
  MIDAS supports frontends of different types, that include the data readout and monitoring
  frontends.
\item 
  the DAQ system described here, [unlike the g-2 DAQ], is not using the event building
  functionality of MIDAS. Instead, the event building is performed by ARTDAQ.
\item 
  Therefore, the architecture of the presented DAQ only requires monitoring
  and slow control frontends.
\item
  frontends can be implemented in C/C++, and Python. MIDAS code distribution
  provides multiple examples and templates of the frontend code.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interface to ARTDAQ}
ARTDAQ handles all high-bandwidth data traffic, which includes reading the
detector data, one-level software trigger, data logging, and event distribution
to online consumers (DQM).
%
ARTDAQ processes are controlled by the Trigger Farm Manager \cite{DAQ_2025_TFM}
which is interfaced to MIDAS via a special TFM frontend \cite{DAQ_2025_FRONTENDS}.

\begin{itemize}
\item 
  TFM reads its initial settings from the "/Mu2e/ActiveRunConfiguration/DAQ/Tfm"
  ODB configuration path.
\item
  FCL files of ARTDAQ processes are stored in \$MU2E\_DAQ\_DIR/config/\$run\_configuration
  subdirectory, where {\bf \$run\_configuration} is the name of the active run
  configuration. The subdirectory is assumed to be the same (network mounted)
  on all DAQ nodes.
  The FCL files could be regenerated manually based on the run configuration
  and the trigger table.
\item
  Archival mechanism: the FCL files used for a given run are stored in \\
  \$DAQ\_OUTPUT\_TOP/run\_records/\$run\_number area of the central DAQ node
  where the TFM frontend is running.
\item
  the log files of the ARTDAQ processes running on a given node, one logfile
  per process per run,
  are stored in \$DAQ\_OUTPUT\_TOP/logs directory of that node
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Communication with ARTRAQ}

\begin{itemize}
\item
  artdaq processes communicate with each only via the data streams, however
  they support an XML-RPC messaging protocol can execute a limited number
  of commands, for example, handle the run transitions. Normally, the commands
  are sent by the farm manager.
\item
  In the default implementation, the ARTDAQ processes are sending their metrics
  via the MessageFacility messaging tool and it was assumed that parsing of
  the metrics, visualizing it and generating alarms is the job of the 3-rd
  party application, i.e. {\bf grafana}.
\item
  The ARTDAQ software was extended to allow querying the ARTDAQ process
  metrics via the XML-RPC and storing that as history information
  in ODB by the node frontends - see Section~\ref{sec:node_frontends}.
  That obsoletes the need in the 3rd party applications. 
\item
  To report their status, Mu2e -specific applications, i.e. the boardreaders
  and the filter modules communicate to the MIDAS server reporting their status 
  via messaging and by storing it in ODB . Therefore simple alarms like single ROC
  timeouts etc could be generated as soon as they are detected by the applications. 
  This avoids the need in additional layers of software to recognize an alarming
  situation and trigger the corresponding notification mechanism.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interface to the online run conditions database} 

\begin{itemize}
\item
  a light-weight python module \cite{DAQ_2025_FRONTENDS} called by the MIDAS sequencer
  interfaces the DAQ system to the online run conditions PSQL DB.
  The communication protocol is as follows:
\item
  the Sequencer requests  a new run number from the run configuration DB
  and stores it in ODB as  {\blue"/Runinfo/Run number"}.
\item
  during the begin run transition, the TFM reads the run number from ODB
  and passes it to ARTDAQ processes via the environment - env var {\blue RUN\_NUMBER}
\item
  the global configuration frontend registers the start and the stop of each
  run transition in the run configuration database (to be checked).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Slow monitoring and interface to EPICS}

The slow monitoring framework is provided by the MIDAS history system.
THis is one of the best technical solutions to the slow monitoring problem possible.
The interface via ODB is simple and crisp:
\begin{itemize}
\item
  all parameters to be monitored are stored by the monitoring frontends in ODB.
\item
  the visualization utilities simply display the content of the predefined ODB locations.
\end{itemize}

Therefore, the visualization and control is completely decoupled from the collection of the
monitoring parameters.

The historical data could be stored in  MIDAS internal format or in a database.
The list of supported databases includes ODBC, SQLITE, MYSQL and PGSQL.

This functionality is similar to that of grafana, however integrated with the
run control system

\begin{itemize}
\item
  one monitoring frontend per node 
  \begin{itemize}
  \item
    controls and configures DTC's and ROCs on this node
  \item
    controls the CFO if the CFO is running on this node
  \item
    monitors DTCs and ROCs, collecting history and non-history information
  \item
    monitors artdaq processes d separate collection of the monitoring information and its visualization
  \end{itemize}
\end{itemize}

Figure ~\ref{figure:slow_controls_node_page} gives an example of one of the slow controls pages

\begin{figure}[H]
  \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] at (0,0.) {
      % \node[shift={(0 cm,0.cm)},inner sep=0,rotate={90}] at (0,0) {}
      \makebox[\textwidth][c] {
        \includegraphics[width=0.95\textwidth]{png/slow_controls_node_page}
      }
    };
    % \node [text width=8cm, scale=1.0] at (14.5,0.5) {$\mu_B$, expected background mean};
    % \node [text width=8cm, scale=1.0, rotate={90}] at (1.5,7.5) { $S_{D}$, ``discovery'' signal strength  };
  \end{tikzpicture}
  \caption{
    \label{figure:slow_controls_node_page}
    A prototype of the DAQ node slow control page. Monitored are the DTC and ROC temperatures and voltages
    and rates of the ARTDAQ processes
  }
\end{figure}


In addition to its own fully implemented history system, MIDAS also has an interface to EPICS.
The interface is implemented as a 
\href{https://bitbucket.org/tmidas/midas/src/develop/examples/epics/}
         {\blue slow control frontend} with the 
\href{https://bitbucket.org/tmidas/midas/src/develop/drivers/device/epics_ca.cxx}
{\blue special EPICS driver}.
%
One therefore can in a transparent way combine EPICS data collection capabilities 
with the MIDAS web display system. MEG-II uses this approach for the beam parameter monitoring.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Alarm system}

MIDAS has a built-in alarm system which could be used to inform shifters
about various categories of failures.

That is not a replacement for critical alarms coming from different inputs.
CRYO ? Accelerator ? -{\red talk to Andy Hocker.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Javascript web interface}

\begin{itemize}
\item 
  MIDAS web interface is a combination of HTML5+Javascript, based on the
  asynchronous approach to the web pages update (Ajax).
\item 
  MIDAS provides Javascript API to ODB, which facilitates development of
  functional experiment-specific {\bf custom} web pages.
  The interface is documented at \\
  \href{https://daq00.triumf.ca/MidasWiki/index.php/Custom\_Page}
  {\blue https://daq00.triumf.ca/MidasWiki/index.php/Custom\_Page}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interfaces to ECL, ELOG, Slack}

MIDAS has interfaces to internal and external ELOGs and Slack.
\begin{itemize}
\item
  interface to the external ELOG has been implemented and tested
\item
  interface to Slack has been tested and used by at least two experiments - MEG and g-2
\item
  interface to ECL - to be implemented
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{DQM}

DQM processes use ROOT-based histogramming. As any ROOT executable is a web server,
the DAQM jobs publish their histograms on the web, and their clients (users)
simply connect to them over the http protocol.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Interprocess communication} 

The system supports three mechanisms
\begin{itemize}
\item
  via ODB : some clients write into ODB, others read
\item
  separate collection of the monitoring information and its visualization
\item
  MIDAS messaging: jsonrpc.
  \begin{itemize}
  \item
    Each client can have a jsonrpc server and receive messages
    from any other client.
  \item
    clients can broadcast messages to the system (info and alarm)
    and communicate to each other
  \item
    each frontend, C++ or Python, has a built-in jsonrpc communication
    functionality built-in.
  \end{itemize}
\item
  communication between ARTDAQ processes is based on an older technology,
  XML-RPC. The TFM frontend supports the XML-RPC-based ARTDAQ communication
  protocol.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Data model ??? } 



